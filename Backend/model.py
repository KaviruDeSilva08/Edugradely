# -*- coding: utf-8 -*-


##Automatically generated by Colab.

##Original file is located at
   ## https://colab.research.google.com/drive/19fsqR-YnjK7dM7lz_CmSl7XkD-eWRkIJ



import pandas as pd

try:
    df = pd.read_csv('auto_grading_dataset.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'auto_grading_dataset.csv' not found.")
    df = None  # Set df to None to indicate failure
except pd.errors.ParserError:
    print("Error: Unable to parse the CSV file. Check the file format.")
    df = None
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    df = None

# Data exploration


# Analyze the distribution of the target variable
print(df['FinalGrade'].describe())
print(df['FinalGrade'].value_counts())

# Investigate data types
print(df.dtypes)

# Check for missing values
print(df.isnull().sum())

# Check for correlations (if applicable) - only include numeric columns
numeric_df = df.select_dtypes(include=['number'])
print(numeric_df.corr())


# Data cleaning


import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download required NLTK resources if not already present
import nltk
try:
    stop_words = stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
    stop_words = stopwords.words('english')
try:
    stemmer = PorterStemmer()
except LookupError:
    nltk.download('punkt')
    stemmer = PorterStemmer()

# 1. Handle missing values
if df['FinalGrade'].isnull().any():
    df = df.dropna(subset=['FinalGrade'])

for col in df.columns:
    if col != 'FinalGrade' and df[col].isnull().sum() > 0:
        if df[col].isnull().sum() / len(df) < 0.1:
          if pd.api.types.is_numeric_dtype(df[col]):
            df[col] = df[col].fillna(df[col].median())
          else:
            df[col] = df[col].fillna(df[col].mode()[0])
        else:
            df = df.drop(columns=[col])

# 2. Clean text data
text_cols = ['Question', 'Answer', 'Rubric', 'Feedback']
for col in text_cols:
    if col in df.columns:
        df[col] = df[col].astype(str).str.lower()
        df[col] = df[col].apply(lambda x: re.sub(r'[^\w\s]', '', x))
        df[col] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
        df[col] = df[col].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))

# 3. Remove duplicate rows
df = df.drop_duplicates(keep='first')

# 4. Verify data types
try:
    df['FinalGrade'] = pd.to_numeric(df['FinalGrade'], errors='coerce')
    df = df.dropna(subset=['FinalGrade'])
except KeyError:
    print("Error: 'FinalGrade' column not found.")
    # Handle the error appropriately, e.g., set a flag or exit.
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    # Handle the error appropriately
display(df)

# Feature engineering

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances
import numpy as np

# 1. Cosine Similarity
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['Answer'] + ' ' + df['Rubric'])
cosine_similarities = cosine_similarity(tfidf_matrix)
# Extract diagonal elements (similarity between Answer and Rubric for each row)
df['cosine_similarity'] = np.diag(cosine_similarities)

# 2. TF-IDF Vectorization and Distances
tfidf_answer = vectorizer.transform(df['Answer'])
tfidf_rubric = vectorizer.transform(df['Rubric'])

# Euclidean distance
euclidean_dist = euclidean_distances(tfidf_answer, tfidf_rubric)
df['euclidean_distance'] = np.diag(euclidean_dist)

# Manhattan distance
manhattan_dist = manhattan_distances(tfidf_answer, tfidf_rubric)
df['manhattan_distance'] = np.diag(manhattan_dist)


# 4. Other features (example: answer length)
df['answer_length'] = df['Answer'].str.len()

display(df.head())

# Data splitting


from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
features = ['cosine_similarity', 'euclidean_distance', 'manhattan_distance', 'answer_length']
X = df[features]
y = df['FinalGrade']


# Split data into training and testing sets directly
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train the model
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")


from sklearn.model_selection import train_test_split

# Split data into training and testing sets directly
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# Print shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)


from sklearn.model_selection import train_test_split

# Split data into training and testing sets directly
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# Print shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Model training

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Define features (X) and target (y)
features = ['cosine_similarity', 'euclidean_distance', 'manhattan_distance', 'answer_length']
X = df[features]
y = df['FinalGrade']


# Split data into training and testing sets directly
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train the model
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Define features (X) and target (y)
features = ['cosine_similarity', 'euclidean_distance', 'manhattan_distance', 'answer_length']
X = df[features]
y = df['FinalGrade']

# Create and train the model
model = RandomForestRegressor(random_state=42)
model.fit(X, y)

# Make predictions on the entire dataset (no separate test set)
y_pred = model.predict(X)

# Evaluate the model
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")

# Model optimization

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Define features (X) and target (y)
features = ['cosine_similarity', 'euclidean_distance', 'manhattan_distance', 'answer_length']
X = df[features]
y = df['FinalGrade']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for RandomizedSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a RandomForestRegressor model
rf_model = RandomForestRegressor(random_state=42)

# Perform RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_grid,
    n_iter=10,  # Number of parameter settings sampled
    scoring='neg_mean_absolute_error',
    cv=5,
    verbose=1,
    random_state=42
)
random_search.fit(X_train, y_train)

# Get the best estimator
best_rf_model = random_search.best_estimator_

# Evaluate the best estimator on the test set
y_pred = best_rf_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Best Hyperparameters: {random_search.best_params_}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Use the best hyperparameters found by RandomizedSearchCV
best_rf_model = RandomForestRegressor(n_estimators=50, min_samples_split=2, min_samples_leaf=4, max_depth=None, random_state=42)

# Train the model on the entire dataset
best_rf_model.fit(X, y)

# Make predictions on the entire dataset
y_pred = best_rf_model.predict(X)

# Evaluate the model
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")

# Model evaluation

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Use the best hyperparameters found by RandomizedSearchCV (from previous steps)
# Assuming best_rf_model is already defined and trained
# If not, retrain the model here with the best hyperparameters

# Define features (X) and target (y)
features = ['cosine_similarity', 'euclidean_distance', 'manhattan_distance', 'answer_length']
X = df[features]
y = df['FinalGrade']

# Evaluate the model on the entire dataset
y_pred = best_rf_model.predict(X)
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")

print("\nLimitations and Potential Improvements:")
print("The dataset is very small, so these evaluation metrics might not be representative of the model's performance on unseen data. The model may be overfitting to the training data. To improve the model:")
print("1. Acquire more data: A larger and more diverse dataset would lead to a more robust and reliable model.")
print("2. Explore alternative algorithms: Other regression algorithms might perform better with this type of data.")
print("3. Feature engineering: Consider adding or transforming features, for example, using more sophisticated text analysis techniques or incorporating information about the guidelines.")


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_percentage_error
import numpy as np

# Define features (X) and target (y)
features = ['cosine_similarity', 'euclidean_distance', 'manhattan_distance', 'answer_length']
X = df[features]
y = df['FinalGrade']

# Create and train the model
model = RandomForestRegressor(random_state=42)
model.fit(X, y)

# Make predictions on the entire dataset
y_pred = model.predict(X)

# Calculate Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y, y_pred) * 100

print(f"Model Accuracy (based on MAPE): {100 - mape:.2f}%")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")


# Use the best hyperparameters found by RandomizedSearchCV (from previous steps)
best_rf_model = RandomForestRegressor(n_estimators=50, min_samples_split=2, min_samples_leaf=4, max_depth=None, random_state=42)

# Train the model on the entire dataset (as done in the last step of model evaluation)
features = ['cosine_similarity', 'euclidean_distance', 'manhattan_distance', 'answer_length']
X = df[features]
y = df['FinalGrade']
best_rf_model.fit(X, y)


# Example input data for prediction (replace with actual guideline and answer)
guideline = "The answer should describe the process of photosynthesis."
answer = "Photosynthesis is the process where plants convert light energy into chemical energy."


# Text cleaning
stop_words = stopwords.words('english')
stemmer = PorterStemmer()

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = ' '.join([word for word in text.split() if word not in stop_words])
    text = ' '.join([stemmer.stem(word) for word in text.split()])
    return text

cleaned_guideline = clean_text(guideline)
cleaned_answer = clean_text(answer)

# Feature Engineering
# Assuming the vectorizer was trained on the 'Answer' + 'Rubric' column
vectorizer = TfidfVectorizer()
vectorizer.fit(df['Answer'] + ' ' + df['Rubric']) # Retrain vectorizer on the combined column

# TF-IDF vectors for the new input
tfidf_guideline = vectorizer.transform([cleaned_guideline])
tfidf_answer_input = vectorizer.transform([cleaned_answer])

# Cosine Similarity
cosine_sim_input = cosine_similarity(tfidf_answer_input, tfidf_guideline)[0][0]

# Euclidean Distance
euclidean_dist_input = euclidean_distances(tfidf_answer_input, tfidf_guideline)[0][0]

# Manhattan Distance
manhattan_dist_input = manhattan_distances(tfidf_answer_input, tfidf_guideline)[0][0]

# Answer Length
answer_length_input = len(cleaned_answer)

# Create a DataFrame for prediction
input_features = pd.DataFrame([[cosine_sim_input, euclidean_dist_input, manhattan_dist_input, answer_length_input]],
                              columns=features)

# Make the prediction
predicted_grade = best_rf_model.predict(input_features)[0]

print(f"Predicted coursework grade: {predicted_grade:.2f}")

# prompt: model need save as pki

import pickle

# Save the trained model to a pickle file
with open('coursework_grading_model.pkl', 'wb') as file:
    pickle.dump(best_rf_model, file)

print("Model saved as coursework_grading_model.pkl")

